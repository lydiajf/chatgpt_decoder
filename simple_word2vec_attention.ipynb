{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a a a a a b b b c c c c c c c d d d d d e e e e e e e f f f g g g g h h h h i i i i i i i j j j j k k k k k l l l l l l l m m m n n n o o o o o o o p p p p p p p q q q r r r r s s s t t t t u u u u v v v v v v v w w w w x x x x x y y y y y y y z z z z z z z\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import collections\n",
    "import more_itertools\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Set device for torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(52)\n",
    "\n",
    "# Define possible repeat lengths (e.g., 'aaa', 'bbbb', 'ccccccc')\n",
    "repeats_range = [3, 4, 5, 7]\n",
    "\n",
    "# Generate `text8` with variable-length sequences for each letter in the alphabet\n",
    "text8 = ' '.join([\n",
    "    ' '.join([char] * random.choice(repeats_range)) for char in string.ascii_lowercase\n",
    "])\n",
    "\n",
    "# Print the generated `text8`\n",
    "print(text8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> list[str]:\n",
    "  text = text.lower()\n",
    "  words = text.split()\n",
    "  stats = collections.Counter(words)\n",
    "  words = [word for word in words if stats[word] > 0]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import collections\n",
    "import more_itertools\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "  text = text.lower()\n",
    "  text = text.replace('.',  ' <PERIOD> ')\n",
    "  text = text.replace(',',  ' <COMMA> ')\n",
    "  text = text.replace('\"',  ' <QUOTATION_MARK> ')\n",
    "  text = text.replace(';',  ' <SEMICOLON> ')\n",
    "  text = text.replace('!',  ' <EXCLAMATION_MARK> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace('(',  ' <LEFT_PAREN> ')\n",
    "  text = text.replace(')',  ' <RIGHT_PAREN> ')\n",
    "  text = text.replace('--', ' <HYPHENS> ')\n",
    "  text = text.replace('?',  ' <QUESTION_MARK> ')\n",
    "  text = text.replace(':',  ' <COLON> ')\n",
    "  words = text.split()\n",
    "  stats = collections.Counter(words)\n",
    "  words = [word for word in words if stats[word] > 5]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text8') as f: text8: str = f.read()\n",
    "titles_string = ' '.join(text8)  # Joining with a space\n",
    "\n",
    "# Concatenate the titles string to the text8 variable\n",
    "text8 += ' ' + titles_string  # Add a space for separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus: list[str] = preprocess(text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as \n"
     ]
    }
   ],
   "source": [
    "print(text8[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "  word_counts = collections.Counter(words)\n",
    "  vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "  int_to_vocab = {ii+1: word for ii, word in enumerate(vocab)}\n",
    "  int_to_vocab[0] = '<PAD>'\n",
    "  vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "  return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary \n",
    "words_to_ids, ids_to_words = create_lookup_tables(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5234"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_ids['anarchism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating token for the list we have from our dictionary \n",
    "tokens = [words_to_ids[word] for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "    def __init__(self, voc, emb, ctx):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "        self.ctx_emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)  # Additional embedding for context\n",
    "        self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "        #self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inpt, trgs):\n",
    "        \n",
    "        # Lets stack these tensors\n",
    "        # input is the tar token 1x1\n",
    "        # trgs is the cw 1x4\n",
    "        print(\"this is the shape inpt\",inpt.shape) #[1]\n",
    "        print(\"this is the shape trgs\",trgs.shape) # [1,4]\n",
    "        \n",
    "        # need to unsqueeze\n",
    "        inpt = inpt.unsqueeze(1)\n",
    "\n",
    "        print(\"this is the shape new inpt\",inpt.shape) #[1,1]\n",
    "\n",
    "        cw_tensor = torch.cat((inpt,trgs) ,dim=1)\n",
    "        \n",
    "        print(\"this is the shape\",cw_tensor.shape)\n",
    "        \n",
    "        #Get embeddings for input and context\n",
    "        emb = self.emb(cw_tensor)  # Shape: (batch_size, emb_dim) -> e.g., (1, 64)\n",
    "        print('shape of emb',emb.shape)\n",
    "\n",
    "        ctx = self.ctx_emb(cw_tensor)  # Shape: (batch_size, num_context_tokens, emb_dim) -> e.g., (1, 4, 64)\n",
    "        print('shape of target words',ctx.shape)\n",
    "        \n",
    "        ctx = ctx.squeeze(0)\n",
    "        \n",
    "        print('shape of ctx',ctx.shape)\n",
    "\n",
    "        \n",
    "        similarity_matrix = torch.matmul(ctx, ctx.T) \n",
    "        print('similarity matrix is', similarity_matrix.shape)\n",
    "\n",
    "        \n",
    "        soft_matrix = torch.nn.functional.softmax(similarity_matrix, dim=1)\n",
    "        \n",
    "        print(\"softmax matrix shape\",soft_matrix.shape)\n",
    "        \n",
    "        attention = torch.matmul(soft_matrix, emb)  #Attention embeddings\n",
    "        \n",
    "        print(\"attention\", attention.shape)\n",
    "\n",
    "        \n",
    "        out = self.ffw(attention[:, 0, :])  # project this\n",
    "        \n",
    "        # out = torch.nn.functional.softmax(out, dim=1)\n",
    "        print(\"show me what out looks like -->\", out)\n",
    "        \n",
    "\n",
    "        # out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "    def __init__(self, voc, emb, ctx):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "        self.ctx_emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)  # Additional embedding for context\n",
    "        self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "        self.linear_q = torch.nn.Linear(64,64)\n",
    "        self.linear_k = torch.nn.Linear(64,64)\n",
    "        self.linear_v = torch.nn.Linear(64,64)\n",
    "        # Add learnable bias for the attention embeddings\n",
    "        self.attn_embedding_bias = torch.nn.Parameter(torch.zeros(emb))\n",
    "\n",
    "    def forward(self, inpt, trgs):\n",
    "        # Stack input and target tensors\n",
    "        # print(\"this is the shape inpt\", inpt.shape)  # [1]\n",
    "        # print(\"this is the shape trgs\", trgs.shape)  # [1,4]\n",
    "        \n",
    "        # Unsqueeze input\n",
    "        inpt = inpt.unsqueeze(1)\n",
    "        # print(\"this is the shape new inpt\", inpt.shape)  # [1,1]\n",
    "        \n",
    "        # Concatenate input and targets\n",
    "        cw_tensor = torch.cat((inpt, trgs), dim=1)\n",
    "        # print(\"this is the shape\", cw_tensor.shape)\n",
    "        \n",
    "        # Get embeddings for input and context\n",
    "        emb = self.emb(cw_tensor)  # Shape: (batch_size, emb_dim) -> e.g., (1, 64)\n",
    "        # print('shape of emb', emb.shape)\n",
    "        \n",
    "        ctx = self.ctx_emb(cw_tensor)  # Shape: (batch_size, num_context_tokens, emb_dim) -> e.g., (1, 4, 64)\n",
    "    \n",
    "        # print('shape of target words', ctx.shape)\n",
    "        \n",
    "        ctx = self.linear_q(ctx)\n",
    "        print(ctx.shape)\n",
    "        # Squeeze context embeddings\n",
    "        ctx = ctx.squeeze(0)\n",
    "        # print('shape of ctx', ctx.shape)\n",
    "        \n",
    "        # Calculate similarity matrix and apply softmax for attention\n",
    "        similarity_matrix = torch.matmul(ctx, ctx.T)\n",
    "        # print('similarity matrix is', similarity_matrix.shape)\n",
    "        \n",
    "        soft_matrix = torch.nn.functional.softmax(similarity_matrix, dim=1)\n",
    "        # print(\"softmax matrix shape\", soft_matrix.shape)\n",
    "        \n",
    "        # Calculate the attention-weighted embeddings and add the bias\n",
    "        attention = torch.matmul(soft_matrix, emb) + self.attn_embedding_bias  # Add bias here\n",
    "        # print(\"attention with bias\", attention.shape)\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        out = self.ffw(attention[:, 0, :])  # Project to output vocabulary space\n",
    "        # print(\"show me what out looks like -->\", out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mFoo 12231808\n"
     ]
    }
   ],
   "source": [
    "args = (len(words_to_ids), 64,2)\n",
    "mFoo = SkipGramFoo(*args)\n",
    "print('mFoo', sum(p.numel() for p in mFoo.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate thing \n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\MLX project\\Tranformer_project\\wandb\\run-20241105_184717-0hhryglo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/word2vec_attention/runs/0hhryglo' target=\"_blank\">bias weighting,with more softmax</a></strong> to <a href='https://wandb.ai/omareweis123/word2vec_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/word2vec_attention' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/word2vec_attention/runs/0hhryglo' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention/runs/0hhryglo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|▏         | 2/96 [00:00<00:05, 18.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|▊         | 8/96 [00:00<00:04, 20.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▍        | 14/96 [00:00<00:03, 22.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  18%|█▊        | 17/96 [00:00<00:03, 23.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  24%|██▍       | 23/96 [00:01<00:03, 23.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|███       | 29/96 [00:01<00:02, 23.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███▎      | 32/96 [00:01<00:02, 23.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|███▉      | 38/96 [00:01<00:02, 22.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  46%|████▌     | 44/96 [00:01<00:02, 24.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  52%|█████▏    | 50/96 [00:02<00:01, 24.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  58%|█████▊    | 56/96 [00:02<00:01, 24.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  61%|██████▏   | 59/96 [00:02<00:01, 24.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  68%|██████▊   | 65/96 [00:02<00:01, 24.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  74%|███████▍  | 71/96 [00:03<00:01, 24.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|████████  | 77/96 [00:03<00:00, 24.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  83%|████████▎ | 80/96 [00:03<00:00, 23.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|████████▉ | 86/96 [00:03<00:00, 23.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  96%|█████████▌| 92/96 [00:03<00:00, 23.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n",
      "torch.Size([1, 5, 64])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6e566fd9fc49dc8064d15827fee13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▆▅█▅▆▄▅▇▅▄▄▅▆▅▅▆█▃▇▅▇▅▄▄▆▇▅█▆▆▃▁▄▅▄▄▄▅▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_loss</td><td>11.09584</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>loss</td><td>10.41993</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bias weighting,with more softmax</strong> at: <a href='https://wandb.ai/omareweis123/word2vec_attention/runs/0hhryglo' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention/runs/0hhryglo</a><br/> View project at: <a href='https://wandb.ai/omareweis123/word2vec_attention' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241105_184717-0hhryglo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import more_itertools\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"word2vec_attention\", entity=\"omareweis123\", name='bias weighting,with more softmax')\n",
    "\n",
    "# Set parameters\n",
    "learning_rate = 0.001  # Define your learning rate\n",
    "mFoo = mFoo.to(device)\n",
    "\n",
    "# Set context size\n",
    "context_size = 2  # Example context size\n",
    "window_size = 2 * context_size + 1  # Total tokens in the window\n",
    "\n",
    "# Initialize the optimizer\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=learning_rate)\n",
    "\n",
    "## Instantiate the CrossEntropyLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    wins = list(more_itertools.windowed(tokens[:100], window_size))  # Convert to list for easier iteration\n",
    "    prgs = tqdm.tqdm(wins, total=len(wins), desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    total_loss = 0.0  # Initialize total loss for the epoch\n",
    "\n",
    "    for win in prgs:\n",
    "        # Prepare input and target tensors for a single window\n",
    "        inpt = torch.LongTensor([win[context_size]]).to(device)  # Central token for the window\n",
    "        trgs = torch.LongTensor([win[:context_size] + win[context_size + 1:]]).to(device)  # Context tokens (left and right)\n",
    "\n",
    "        # Zero gradients\n",
    "        opFoo.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        out = mFoo(inpt, trgs)\n",
    "        \n",
    "        # Assuming `out` is the output logits from the model\n",
    "        true_index = torch.LongTensor([win[context_size]]).to(device)  # The index of the central token\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(out, true_index)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        opFoo.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Log the loss\n",
    "        wandb.log({'loss': loss.item(), 'learning_rate': learning_rate})\n",
    "\n",
    "    # Calculate and log average loss for the epoch\n",
    "    average_loss = total_loss / len(wins) if len(wins) > 0 else 0\n",
    "    wandb.log({'average_loss': average_loss})\n",
    "\n",
    "# Finish the W&B logging\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4148,  159,   50, 6438]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mFoo(mFoo.emb(torch.tensor(1)), mFoo.emb(torch.tensor(1)))\n",
    "v1 = mFoo.emb(torch.tensor(1))\n",
    "v2 = mFoo.emb(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = mFoo(inpt, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84])\n",
      "tensor([[4148,  159,   50, 6438]])\n"
     ]
    }
   ],
   "source": [
    "print (inpt)\n",
    "print (trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 63642])\n",
      "tensor(7.9093, grad_fn=<UnbindBackward0>)\n",
      "tensor([[-9.3935, -9.5191, -9.6235,  ..., -8.9609, -9.7295, -9.7800]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5234"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (out.shape) # Vocabulary Size - this output contains the probabiltiy distribution after the activation function.\n",
    "''' This means we are missing the activation function in the model.'''\n",
    "print (max(out[0])) # This is the token selected \n",
    "print (out)\n",
    "words_to_ids['anarchism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(v1.shape)\n",
    "print(v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "v22 = torch.unsqueeze(v2, 1)\n",
    "print(v22.T.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'unsqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmFoo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv222\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# v222 = torch.unsqueeze(v2, 1).T\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[22], line 19\u001b[0m, in \u001b[0;36mSkipGramFoo.forward\u001b[1;34m(self, inpt, trgs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt, trgs):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Stack input and target tensors\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# print(\"this is the shape inpt\", inpt.shape)  # [1]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# print(\"this is the shape trgs\", trgs.shape)  # [1,4]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Unsqueeze input\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     inpt \u001b[38;5;241m=\u001b[39m \u001b[43minpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# print(\"this is the shape new inpt\", inpt.shape)  # [1,1]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Concatenate input and targets\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     cw_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((inpt, trgs), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'unsqueeze'"
     ]
    }
   ],
   "source": [
    "mFoo(, v222)\n",
    "# v222 = torch.unsqueeze(v2, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
