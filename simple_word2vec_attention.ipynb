{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'z b s m y k x e z k e i q a k a b e f h u k w x x y o r w o d n n e c e s v r l a a r n l e d e d p h o l o t u d y w v l y v g r w t x n r b r d s x j b b n e t p n r d l b o u e r z i f g v c u g t'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import collections\n",
    "import more_itertools\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(52)\n",
    "\n",
    "# Generate a list of random letters (a-z)\n",
    "letters = random.choices(string.ascii_lowercase, k=100)\n",
    "\n",
    "# Join the letters with whitespace\n",
    "text8 = ' '.join(letters)\n",
    "text8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> list[str]:\n",
    "  text = text.lower()\n",
    "  words = text.split()\n",
    "  stats = collections.Counter(words)\n",
    "  words = [word for word in words if stats[word] > 0]\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus: list[str] = preprocess(text8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(words: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "  word_counts = collections.Counter(words)\n",
    "  vocab = sorted(word_counts, key=lambda k: word_counts.get(k), reverse=True)\n",
    "  int_to_vocab = {ii+1: word for ii, word in enumerate(vocab)}\n",
    "  int_to_vocab[0] = '<PAD>'\n",
    "  vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "  return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary \n",
    "words_to_ids, ids_to_words = create_lookup_tables(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating token for the list we have from our dictionary \n",
    "tokens = [words_to_ids[word] for word in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramFoo(torch.nn.Module):\n",
    "  def __init__(self, voc, emb, ctx):\n",
    "    super().__init__()\n",
    "    self.ctx = ctx\n",
    "    self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "    self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "    self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "#new forwarding for batch size \n",
    "  def forward(self, inpt, trgs):\n",
    "    \n",
    "    emb = self.emb(inpt)\n",
    "    batch_size = inpt.size(0)  \n",
    "    \n",
    "    ctx = self.ffw.weight[trgs.to(inpt.device)]  \n",
    "    \n",
    "    assert ctx.size(0) == emb.size(0), f\"Context batch size {ctx.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "    # Perform batch matrix multiplication\n",
    "    print(emb.shape)\n",
    "    out = torch.bmm(ctx, emb.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, 2)\n",
    "    \n",
    "    print(out.shape)\n",
    "    # Apply sigmoid and clamp to prevent NaNs\n",
    "    out = self.sig(out).clamp(min=1e-7, max=1 - 1e-7)\n",
    "    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# class SkipGramFoo(torch.nn.Module):\n",
    "#     def __init__(self, voc, emb, ctx):\n",
    "#         super().__init__()\n",
    "#         self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "#         self.ctx_emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)  # Additional embedding for context\n",
    "#         self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "#         self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, inpt, trgs):\n",
    "#         # Get embeddings for input and context\n",
    "#         emb = self.emb(inpt)\n",
    "#         print(emb.shape)\n",
    "#         ctx = self.ctx_emb(trgs)\n",
    "#         print(ctx.shape)  # Get context embeddings\n",
    "        \n",
    "#         # Ensure dimensions match for operations\n",
    "#         assert ctx.size(0) == emb.size(0), f\"Context batch size {ctx.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "        \n",
    "#         # Perform batch matrix multiplication\n",
    "#         # (batch_size, emb_dim) * (batch_size, vocab_size) -> (batch_size, vocab_size)\n",
    "#         context_matrix = torch.bmm(ctx, emb) # Change this if necessary\n",
    "#         print('context matrix is', context_matrix.shape)\n",
    "#         similarity_matrix = torch.matmul(context_matrix, context_matrix.T) \n",
    "#         print('similarity matrix is', similarity_matrix.shape)\n",
    "#         soft_matrix = torch.nn.functional.softmax(similarity_matrix,dim=1)\n",
    "    \n",
    "\n",
    "#         return soft_matrix\n",
    "import torch\n",
    "\n",
    "class SkipGramFoo(torch.nn.Module):\n",
    "    def __init__(self, voc, emb, ctx):\n",
    "        super().__init__()\n",
    "        self.emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)\n",
    "        self.ctx_emb = torch.nn.Embedding(num_embeddings=voc, embedding_dim=emb)  # Additional embedding for context\n",
    "        self.ffw = torch.nn.Linear(in_features=emb, out_features=voc, bias=False)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inpt, trgs):\n",
    "        # Get embeddings for input and context\n",
    "        emb = self.emb(inpt)  # Shape: (batch_size, emb_dim) -> e.g., (1, 64)\n",
    "        print('shape of emb',emb.shape)\n",
    "        ctx = self.ctx_emb(trgs)  # Shape: (batch_size, num_context_tokens, emb_dim) -> e.g., (1, 4, 64)\n",
    "        print('shape of target words',ctx.shape)\n",
    "        \n",
    "        # Ensure dimensions match for operations\n",
    "        assert ctx.size(2) == emb.size(1), f\"Context batch size {ctx.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "        \n",
    "        # Reshape emb to (batch_size, emb_dim, 1) to make it compatible for bmm\n",
    "        ctx = ctx.squeeze(0)  # Shape: (1, 64, 1)\n",
    "\n",
    "        assert ctx.size(1) == emb.size(1), f\"Context batch size {ctx.size(0)} doesn't match embeddings batch size {emb.size(0)}\"\n",
    "\n",
    "        # Perform batch matrix multiplication\n",
    "        # (batch_size, num_context_tokens, emb_dim) * (batch_size, emb_dim, 1) -> (batch_size, num_context_tokens, 1)\n",
    "        context_matrix = torch.cat((emb,ctx) ,dim=0) # Resulting shape: (1, 4, 1)\n",
    "        print('context matrix is', context_matrix.shape)\n",
    "\n",
    "        # Compute similarity matrix for the context matrix\n",
    "        similarity_matrix = torch.matmul(context_matrix, context_matrix.T) \n",
    "        print('similarity matrix is', similarity_matrix.shape)\n",
    "\n",
    "        # Apply softmax over similarity matrix\n",
    "        soft_matrix = torch.nn.functional.softmax(similarity_matrix, dim=1)\n",
    "    \n",
    "        return soft_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mFoo 5184\n"
     ]
    }
   ],
   "source": [
    "args = (len(words_to_ids), 64,2)\n",
    "mFoo = SkipGramFoo(*args)\n",
    "print('mFoo', sum(p.numel() for p in mFoo.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate thing \n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lsuupb7u) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e505758a2124e10a7fd68e2d6ef2dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.005 MB of 0.005 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">no_batching, tokens1000000000, 30epochs, titlesadded</strong> at: <a href='https://wandb.ai/omareweis123/word2vec_attention/runs/lsuupb7u' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention/runs/lsuupb7u</a><br/> View project at: <a href='https://wandb.ai/omareweis123/word2vec_attention' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241104_190358-lsuupb7u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lsuupb7u). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\omare\\Desktop\\MLX project\\Tranformer_project\\wandb\\run-20241104_190747-pspifs6a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/omareweis123/word2vec_attention/runs/pspifs6a' target=\"_blank\">no_batching, tokens1000000000, 30epochs, titlesadded</a></strong> to <a href='https://wandb.ai/omareweis123/word2vec_attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/omareweis123/word2vec_attention' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/omareweis123/word2vec_attention/runs/pspifs6a' target=\"_blank\">https://wandb.ai/omareweis123/word2vec_attention/runs/pspifs6a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1, 4])\n",
      "shape of emb torch.Size([1, 64])\n",
      "shape of target words torch.Size([1, 4, 64])\n",
      "context matrix is torch.Size([5, 64])\n",
      "similarity matrix is torch.Size([5, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m mFoo(inpt, trgs)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m opFoo\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Accumulate loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:190\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    186\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[0;32m    189\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 190\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\omare\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mones_like(out, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format))\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import more_itertools\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"word2vec_attention\", entity=\"omareweis123\", name='no_batching, tokens1000000000, 30epochs, titlesadded')\n",
    "\n",
    "# Set parameters\n",
    "learning_rate = 0.001  # Define your learning rate\n",
    "mFoo = mFoo.to(device)\n",
    "\n",
    "# Set context size\n",
    "context_size = 2  # Example context size\n",
    "window_size = 2 * context_size + 1  # Total tokens in the window\n",
    "\n",
    "# Initialize the optimizer\n",
    "opFoo = torch.optim.Adam(mFoo.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    wins = list(more_itertools.windowed(tokens[:1000000000], window_size))  # Convert to list for easier iteration\n",
    "    prgs = tqdm.tqdm(wins, total=len(wins), desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "    total_loss = 0.0  # Initialize total loss for the epoch\n",
    "\n",
    "    for win in prgs:\n",
    "        # Prepare input and target tensors for a single window\n",
    "        inpt = torch.LongTensor([win[context_size]]).to(device)  # Central token for the window\n",
    "        trgs = torch.LongTensor([win[:context_size] + win[context_size + 1:]]).to(device)  # Context tokens (left and right)\n",
    "\n",
    "        print(inpt.shape)\n",
    "        print(trgs.shape)\n",
    "\n",
    "        # Zero gradients\n",
    "        opFoo.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = mFoo(inpt, trgs)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        opFoo.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Log the loss\n",
    "        wandb.log({'loss': loss.item(), 'learning_rate': learning_rate})\n",
    "\n",
    "    # Calculate and log average loss for the epoch\n",
    "    average_loss = total_loss / len(wins) if len(wins) > 0 else 0\n",
    "    wandb.log({'average_loss': average_loss})\n",
    "\n",
    "# Finish the W&B logging\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
